"""
Reference Analysis Service (Stage 1)

ë ˆí¼ëŸ°ìŠ¤ ì˜ìƒ(.mp4)ì„ ë¶„ì„í•˜ì—¬ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë™ê¸°í™”ëœ 
Audio/Video íŠ¹ì„±ì„ ì¶”ì¶œí•˜ê³  AnalysisResult ìŠ¤í‚¤ë§ˆë¡œ ì €ì¥í•©ë‹ˆë‹¤.

í•µì‹¬ íŠ¹ì§•:
- Frame-Locked Audio: ì˜¤ë””ì˜¤ íŠ¹ì„±ì´ ë¹„ë””ì˜¤ í”„ë ˆì„ íƒ€ì„ìŠ¤íƒ¬í”„ì— ì •í™•íˆ ì •ë ¬ë¨
- librosa + ffmpeg: Windows í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì •ì ì¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬
- MediaPipe Face Mesh: 478ê°œ ëœë“œë§ˆí¬ ê¸°ë°˜ ë¸”ë Œë“œì‰ì… ê³„ì‚°
"""

from __future__ import annotations

import os
import subprocess
import tempfile
from pathlib import Path
from typing import Optional

import cv2
import librosa
import mediapipe as mp
import numpy as np
import torch

from app.core.schemas import (
    AnalysisResult,
    AudioFeatures,
    Blendshapes,
    FrameData,
    VideoFeatures,
)


class ReferenceAnalysisService:
    """
    ë ˆí¼ëŸ°ìŠ¤ ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤.
    
    MP4 íŒŒì¼ì„ ì…ë ¥ë°›ì•„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë™ê¸°í™”ëœ Audio/Video íŠ¹ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """

    # ì˜¤ë””ì˜¤ ì„¤ì •
    TARGET_SAMPLE_RATE = 22050  # librosa ê¸°ë³¸ê°’, ìŒì„± ë¶„ì„ì— ì í•©
    MFCC_N_COEFFS = 13         # DTWì— ì‚¬ìš©í•  MFCC ê³„ìˆ˜ ê°œìˆ˜
    N_FFT = 2048               # FFT ìœˆë„ìš° í¬ê¸°
    
    # MediaPipe ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (478ê°œ ì¤‘ í•µì‹¬)
    # ì°¸ì¡°: https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png
    LM = {
        # ì…
        "upperLipTop": 13,
        "lowerLipBottom": 14,
        "mouthLeft": 61,
        "mouthRight": 291,
        "upperLipCenter": 0,
        # ëˆˆ
        "leftEyeTop": 159,
        "leftEyeBottom": 145,
        "rightEyeTop": 386,
        "rightEyeBottom": 374,
        "leftEyeInner": 133,
        "leftEyeOuter": 33,
        "rightEyeInner": 362,
        "rightEyeOuter": 263,
        # ëˆˆì¹
        "leftBrowInner": 107,
        "rightBrowInner": 336,
        "leftBrowOuter": 70,
        "rightBrowOuter": 300,
        "browCenter": 9,
        # ì–¼êµ´ ê¸°ì¤€ì 
        "faceLeft": 234,
        "faceRight": 454,
        "faceTop": 10,
        "faceBottom": 152,
        # ì½”
        "noseLeft": 129,
        "noseRight": 358,
        "noseTip": 1,
        # ë³¼
        "leftCheek": 50,
        "rightCheek": 280,
        # ëˆˆë™ì (refine_landmarks=True í•„ìš”)
        "leftPupil": 468,
        "rightPupil": 473,
    }

    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë° MediaPipe ë¡œë“œ."""
        print("ğŸ“Š ReferenceAnalysisService: ì´ˆê¸°í™” ì¤‘...")
        
        # MediaPipe Face Mesh ì„¤ì •
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,  # ë¹„ë””ì˜¤ ëª¨ë“œ (íŠ¸ë˜í‚¹ í™œì„±í™”)
            max_num_faces=1,
            refine_landmarks=True,    # ëˆˆë™ì ëœë“œë§ˆí¬ í™œì„±í™”
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
        )
        
        # PyTorch ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“Š ReferenceAnalysisService: ì´ˆê¸°í™” ì™„ë£Œ! (Device: {self.device})")

    def analyze(
        self,
        video_path: str | Path,
        actor_id: str,
        output_path: Optional[str | Path] = None,
    ) -> AnalysisResult:
        """
        ì˜ìƒì„ ë¶„ì„í•˜ì—¬ AnalysisResult ë°˜í™˜.
        
        Args:
            video_path: ì…ë ¥ MP4 íŒŒì¼ ê²½ë¡œ
            actor_id: ë°°ìš°/ì˜ìƒ ê³ ìœ  ID
            output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
            
        Returns:
            AnalysisResult: ë¶„ì„ ê²°ê³¼
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        print(f"ğŸ¬ ë¶„ì„ ì‹œì‘: {video_path.name}")

        # 1. ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        fps, total_frames, duration_sec = self._get_video_metadata(video_path)

        print(f"   ğŸ“¹ FPS: {fps:.2f}, í”„ë ˆì„: {total_frames}, ê¸¸ì´: {duration_sec:.2f}ì´ˆ")

        # 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° í”„ë ˆì„ ë‹¨ìœ„ íŠ¹ì„± ê³„ì‚°
        audio_features_list = self._extract_audio_features(
            video_path, fps, total_frames
        )

        # 3. ë¹„ë””ì˜¤ í”„ë ˆì„ ë‹¨ìœ„ ì²˜ë¦¬
        video_features_list = self._extract_video_features(video_path)

        # 4. í”„ë ˆì„ ë°ì´í„° ë³‘í•© (ê¸¸ì´ ë§ì¶”ê¸°)
        frames = self._merge_features(
            video_features_list, audio_features_list, fps
        )

        # 5. AnalysisResult ìƒì„±
        result = AnalysisResult(
            actor_id=actor_id,
            duration_sec=duration_sec,
            fps=fps,
            sampling_rate=self.TARGET_SAMPLE_RATE,
            source_file=video_path.name,
            mfcc_n_coeffs=self.MFCC_N_COEFFS,
            audio_hop_length=self._calculate_hop_length(fps),
            frames=frames,
        )

        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(frames)} í”„ë ˆì„")

        # 6. ì €ì¥ (ì„ íƒì )
        if output_path:
            result.save(output_path)
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

        return result

    # =========================================================================
    # ì˜¤ë””ì˜¤ ì²˜ë¦¬
    # =========================================================================

    def _calculate_hop_length(self, fps: float) -> int:
        """
        ë¹„ë””ì˜¤ FPSì— ë§ì¶˜ hop_length ê³„ì‚°.
        
        hop_length = sample_rate / fps
        ì´ë ‡ê²Œ í•˜ë©´ ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ == ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜
        """
        return int(self.TARGET_SAMPLE_RATE / fps)

    def _extract_audio_features(
        self,
        video_path: Path,
        fps: float,
        total_frames: int,
    ) -> list[AudioFeatures]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³  í”„ë ˆì„ ë‹¨ìœ„ íŠ¹ì„± ê³„ì‚°.
        
        torchaudioë¡œ ë¹ ë¥´ê²Œ ë¡œë“œ í›„, í”„ë ˆì„ ë‹¨ìœ„ë¡œ MFCC/Pitch/Energy ì¶”ì¶œ.
        """
        print("   ğŸ”Š ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")

        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ (ffmpeg + librosa - Windows í˜¸í™˜)
        audio_np, original_sr = self._load_audio_from_video(video_path)

        # 2. ë¦¬ìƒ˜í”Œë§ (í•„ìš”ì‹œ librosa ì‚¬ìš©)
        if original_sr != self.TARGET_SAMPLE_RATE:
            audio_np = librosa.resample(
                audio_np,
                orig_sr=original_sr,
                target_sr=self.TARGET_SAMPLE_RATE,
            )

        # 3. Frame-Locked íŠ¹ì„± ì¶”ì¶œ
        hop_length = self._calculate_hop_length(fps)
        
        # MFCC (librosa - ë” ì •í™•í•œ ê²°ê³¼)
        mfcc = librosa.feature.mfcc(
            y=audio_np,
            sr=self.TARGET_SAMPLE_RATE,
            n_mfcc=self.MFCC_N_COEFFS,
            n_fft=self.N_FFT,
            hop_length=hop_length,
        )  # shape: (n_mfcc, n_frames)

        # RMS Energy
        rms = librosa.feature.rms(
            y=audio_np,
            frame_length=self.N_FFT,
            hop_length=hop_length,
        )[0]  # shape: (n_frames,)

        # Pitch (F0) - pyin ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (ì •í™•ë„ ë†’ìŒ)
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio_np,
            fmin=librosa.note_to_hz('C2'),   # 65Hz (ë‚®ì€ ë‚¨ì„± ìŒì„±)
            fmax=librosa.note_to_hz('C6'),   # 1047Hz (ë†’ì€ ì—¬ì„± ìŒì„±)
            sr=self.TARGET_SAMPLE_RATE,
            hop_length=hop_length,
            fill_na=0.0,  # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
        )

        # 4. AudioFeatures ë¦¬ìŠ¤íŠ¸ ìƒì„±
        n_audio_frames = mfcc.shape[1]
        audio_features = []

        for i in range(min(n_audio_frames, total_frames)):
            features = AudioFeatures(
                mfcc=mfcc[:, i].tolist(),
                pitch=float(f0[i]) if f0[i] is not None and not np.isnan(f0[i]) else 0.0,
                energy=float(rms[i]) if i < len(rms) else 0.0,
                pitch_confidence=float(voiced_probs[i]) if voiced_probs[i] is not None else 0.0,
                is_voiced=bool(voiced_flag[i]) if voiced_flag[i] is not None else False,
            )
            audio_features.append(features)

        # ë¶€ì¡±í•œ í”„ë ˆì„ íŒ¨ë”©
        while len(audio_features) < total_frames:
            audio_features.append(AudioFeatures(
                mfcc=[0.0] * self.MFCC_N_COEFFS,
                pitch=0.0,
                energy=0.0,
            ))

        print(f"   ğŸ”Š ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(audio_features)} í”„ë ˆì„")
        return audio_features

    def _load_audio_from_video(self, video_path: Path) -> tuple[np.ndarray, int]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (Windows í˜¸í™˜ - ffmpeg + librosa).
        
        Returns:
            (audio_numpy_array, sample_rate)
        """
        temp_wav = None

        try:
            # ë°©ë²• 1: ffmpegë¡œ WAV ì¶”ì¶œ í›„ librosaë¡œ ë¡œë“œ (ê°€ì¥ ì•ˆì •ì )
            temp_wav = tempfile.mktemp(suffix=".wav")
            
            # subprocess ì‚¬ìš© (Windows í˜¸í™˜ì„±)
            ffmpeg_cmd = [
                'ffmpeg', '-y', '-i', str(video_path),
                '-vn', '-acodec', 'pcm_s16le',
                '-ar', str(self.TARGET_SAMPLE_RATE), '-ac', '1',
                temp_wav, '-loglevel', 'error'
            ]
            
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True)
            except (subprocess.CalledProcessError, FileNotFoundError):
                # ffmpeg ëª…ë ¹ì–´ í˜•íƒœë¡œ ì¬ì‹œë„ (PATH ë¬¸ì œ ëŒ€ì‘)
                os.system(
                    f'ffmpeg -y -i "{video_path}" -vn -acodec pcm_s16le '
                    f'-ar {self.TARGET_SAMPLE_RATE} -ac 1 "{temp_wav}" '
                    f'-loglevel error'
                )

            if os.path.exists(temp_wav) and os.path.getsize(temp_wav) > 0:
                audio_np, sr = librosa.load(temp_wav, sr=self.TARGET_SAMPLE_RATE, mono=True)
                return audio_np, sr

            # ë°©ë²• 2: librosa ì§ì ‘ ì‚¬ìš© (ì¼ë¶€ í¬ë§·ë§Œ ì§€ì›)
            audio_np, sr = librosa.load(str(video_path), sr=None, mono=True)
            return audio_np, sr

        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if temp_wav and os.path.exists(temp_wav):
                try:
                    os.remove(temp_wav)
                except OSError:
                    pass

    # =========================================================================
    # ë¹„ë””ì˜¤ ì²˜ë¦¬
    # =========================================================================

    def _get_video_metadata(self, video_path: Path) -> tuple[float, int, float]:
        """
        ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        WebM ë“± ì¼ë¶€ í¬ë§·ì—ì„œëŠ” CAP_PROP_FRAME_COUNTê°€ -1ì´ë‚˜ ë¹„ì •ìƒì ì¸ ê°’ì„
        ë°˜í™˜í•˜ë¯€ë¡œ, í•„ìš”ì‹œ ì§ì ‘ í”„ë ˆì„ì„ ì„¸ê±°ë‚˜ ffprobeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        Returns:
            (fps, total_frames, duration_sec)
        """
        cap = cv2.VideoCapture(str(video_path))
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # FPS ê¸°ë³¸ê°’ ì²˜ë¦¬
        if fps <= 0 or fps > 240:  # ë¹„ì •ìƒì ì¸ FPS
            fps = 30.0
            print(f"   âš ï¸  FPSë¥¼ ê°ì§€í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ {fps}ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # í”„ë ˆì„ ìˆ˜ê°€ ë¹„ì •ìƒì ì¸ ê²½ìš° (WebM ë“±ì—ì„œ ë°œìƒ)
        if total_frames <= 0:
            print(f"   âš ï¸  í”„ë ˆì„ ìˆ˜ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ì–´ ì§ì ‘ ê³„ì‚°í•©ë‹ˆë‹¤...")
            
            # ë°©ë²• 1: ffprobe ì‚¬ìš© ì‹œë„ (ë” ë¹ ë¦„)
            duration_sec = self._get_duration_with_ffprobe(video_path)
            
            if duration_sec and duration_sec > 0:
                total_frames = int(duration_sec * fps)
            else:
                # ë°©ë²• 2: ì§ì ‘ í”„ë ˆì„ ì„¸ê¸° (ëŠë¦¬ì§€ë§Œ í™•ì‹¤í•¨)
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                total_frames = 0
                while True:
                    ret, _ = cap.read()
                    if not ret:
                        break
                    total_frames += 1
                
                duration_sec = total_frames / fps if fps > 0 else 0
        else:
            duration_sec = total_frames / fps if fps > 0 else 0
        
        cap.release()
        
        # ìµœì¢… ê²€ì¦
        if duration_sec <= 0:
            raise ValueError(f"ì˜ìƒ ê¸¸ì´ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (fps={fps}, frames={total_frames})")
        
        return fps, total_frames, duration_sec

    def _get_duration_with_ffprobe(self, video_path: Path) -> float | None:
        """
        ffprobeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ê¸¸ì´ ì¶”ì¶œ.
        
        Returns:
            duration in seconds, or None if failed
        """
        try:
            cmd = [
                'ffprobe', '-v', 'error',
                '-show_entries', 'format=duration',
                '-of', 'default=noprint_wrappers=1:nokey=1',
                str(video_path)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                return float(result.stdout.strip())
        except (subprocess.TimeoutExpired, FileNotFoundError, ValueError):
            pass
        
        return None

    def _extract_video_features(self, video_path: Path) -> list[VideoFeatures]:
        """
        ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ë‹¨ìœ„ ë¸”ë Œë“œì‰ì… ì¶”ì¶œ.
        """
        print("   ğŸ‘¤ ë¹„ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")

        cap = cv2.VideoCapture(str(video_path))
        video_features = []
        frame_idx = 0

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # BGR -> RGB
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipe ì²˜ë¦¬
            results = self.face_mesh.process(rgb_frame)

            if results.multi_face_landmarks:
                landmarks = results.multi_face_landmarks[0].landmark
                blendshapes = self._calculate_blendshapes(landmarks)
                
                features = VideoFeatures(
                    blendshapes=blendshapes,
                    face_detected=True,
                )
            else:
                features = VideoFeatures(
                    blendshapes=None,
                    face_detected=False,
                )

            video_features.append(features)
            frame_idx += 1

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (100í”„ë ˆì„ë§ˆë‹¤)
            if frame_idx % 100 == 0:
                print(f"      ì²˜ë¦¬ ì¤‘: {frame_idx} í”„ë ˆì„...")

        cap.release()
        print(f"   ğŸ‘¤ ë¹„ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(video_features)} í”„ë ˆì„")
        return video_features

    def _calculate_blendshapes(self, landmarks) -> Blendshapes:
        """
        MediaPipe 478ê°œ ëœë“œë§ˆí¬ì—ì„œ ARKit ìŠ¤íƒ€ì¼ ë¸”ë Œë“œì‰ì… ê³„ì‚°.
        
        ëª¨ë“  ê°’ì€ 0.0 ~ 1.0 ìœ¼ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤.
        """
        def get_point(idx: int) -> np.ndarray:
            """ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ì—ì„œ (x, y, z) ì¢Œí‘œ ë°˜í™˜."""
            lm = landmarks[idx]
            return np.array([lm.x, lm.y, lm.z])

        def dist(idx1: int, idx2: int) -> float:
            """ë‘ ëœë“œë§ˆí¬ ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬."""
            return float(np.linalg.norm(get_point(idx1) - get_point(idx2)))

        def dist_y(idx1: int, idx2: int) -> float:
            """ë‘ ëœë“œë§ˆí¬ ì‚¬ì´ì˜ Yì¶• ê±°ë¦¬ (ìˆ˜ì§)."""
            return abs(landmarks[idx1].y - landmarks[idx2].y)

        def dist_x(idx1: int, idx2: int) -> float:
            """ë‘ ëœë“œë§ˆí¬ ì‚¬ì´ì˜ Xì¶• ê±°ë¦¬ (ìˆ˜í‰)."""
            return abs(landmarks[idx1].x - landmarks[idx2].x)

        # ì–¼êµ´ ê¸°ì¤€ í¬ê¸° (ì •ê·œí™”ìš©)
        face_width = dist(self.LM["faceLeft"], self.LM["faceRight"])
        face_height = dist(self.LM["faceTop"], self.LM["faceBottom"])
        
        if face_width < 1e-6 or face_height < 1e-6:
            return Blendshapes()  # ê¸°ë³¸ê°’ ë°˜í™˜

        # ëˆˆ ê¸°ì¤€ í¬ê¸°
        left_eye_width = dist(self.LM["leftEyeInner"], self.LM["leftEyeOuter"])
        right_eye_width = dist(self.LM["rightEyeInner"], self.LM["rightEyeOuter"])

        # =====================================================================
        # ë¸”ë Œë“œì‰ì… ê³„ì‚° (ARKit ìŠ¤íƒ€ì¼)
        # =====================================================================

        # --- ì… ê´€ë ¨ ---
        jaw_open = self._clamp(
            dist_y(self.LM["upperLipTop"], self.LM["lowerLipBottom"]) / face_height * 5.0
        )

        mouth_width = dist(self.LM["mouthLeft"], self.LM["mouthRight"])
        base_mouth_width = face_width * 0.35  # ê¸°ë³¸ ì… ë„ˆë¹„ ë¹„ìœ¨
        
        # ì›ƒìŒ (ì…ê¼¬ë¦¬ê°€ ì˜¬ë¼ê°€ê³  ì…ì´ ë„“ì–´ì§)
        mouth_left_y = landmarks[self.LM["mouthLeft"]].y
        mouth_right_y = landmarks[self.LM["mouthRight"]].y
        nose_tip_y = landmarks[self.LM["noseTip"]].y
        
        smile_left = self._clamp(
            (nose_tip_y - mouth_left_y) / face_height * 8.0
        )
        smile_right = self._clamp(
            (nose_tip_y - mouth_right_y) / face_height * 8.0
        )
        
        # ì°¡ê·¸ë¦¼ (ì…ê¼¬ë¦¬ê°€ ë‚´ë ¤ê°)
        frown_left = self._clamp(
            (mouth_left_y - nose_tip_y) / face_height * 8.0
        )
        frown_right = self._clamp(
            (mouth_right_y - nose_tip_y) / face_height * 8.0
        )

        # ì… ì˜¤ë¯€ë¦¼ (ì… ë„ˆë¹„ê°€ ì¢ì•„ì§)
        mouth_pucker = self._clamp(
            (base_mouth_width - mouth_width) / base_mouth_width * 2.0
        )

        # ì… ì¢Œìš° ì´ë™
        mouth_center_x = (landmarks[self.LM["mouthLeft"]].x + landmarks[self.LM["mouthRight"]].x) / 2
        face_center_x = (landmarks[self.LM["faceLeft"]].x + landmarks[self.LM["faceRight"]].x) / 2
        mouth_offset = (mouth_center_x - face_center_x) / face_width
        
        mouth_left = self._clamp(-mouth_offset * 5.0) if mouth_offset < 0 else 0.0
        mouth_right = self._clamp(mouth_offset * 5.0) if mouth_offset > 0 else 0.0

        # --- ëˆˆì¹ ê´€ë ¨ ---
        brow_inner_up = self._clamp(
            (landmarks[self.LM["browCenter"]].y - 
             (landmarks[self.LM["leftBrowInner"]].y + landmarks[self.LM["rightBrowInner"]].y) / 2) 
            / face_height * 10.0
        )
        
        # ëˆˆì¹ ë‚´ë¦¼ (ì°¡ê·¸ë¦¼)
        left_brow_down = self._clamp(
            (landmarks[self.LM["leftBrowInner"]].y - landmarks[self.LM["browCenter"]].y)
            / face_height * 10.0
        )
        right_brow_down = self._clamp(
            (landmarks[self.LM["rightBrowInner"]].y - landmarks[self.LM["browCenter"]].y)
            / face_height * 10.0
        )

        # ëˆˆì¹ ë°”ê¹¥ ì˜¬ë¦¼
        left_brow_outer_up = self._clamp(
            (landmarks[self.LM["faceTop"]].y - landmarks[self.LM["leftBrowOuter"]].y)
            / face_height * 5.0
        )
        right_brow_outer_up = self._clamp(
            (landmarks[self.LM["faceTop"]].y - landmarks[self.LM["rightBrowOuter"]].y)
            / face_height * 5.0
        )

        # --- ëˆˆ ê´€ë ¨ ---
        left_eye_open = dist_y(self.LM["leftEyeTop"], self.LM["leftEyeBottom"])
        right_eye_open = dist_y(self.LM["rightEyeTop"], self.LM["rightEyeBottom"])
        
        # ëˆˆ í¬ê²Œ ëœ¸
        eye_wide_left = self._clamp(left_eye_open / left_eye_width * 2.0 - 0.3)
        eye_wide_right = self._clamp(right_eye_open / right_eye_width * 2.0 - 0.3)

        # ëˆˆ ì°¡ê·¸ë¦¼ (ëˆˆì´ ê°€ëŠ˜ì–´ì§)
        eye_squint_left = self._clamp(1.0 - left_eye_open / left_eye_width * 3.0)
        eye_squint_right = self._clamp(1.0 - right_eye_open / right_eye_width * 3.0)

        # ëˆˆ ê°ê¸°
        base_eye_open = left_eye_width * 0.25
        eye_blink_left = self._clamp(1.0 - left_eye_open / base_eye_open)
        eye_blink_right = self._clamp(1.0 - right_eye_open / base_eye_open)

        # --- ë³¼/ì½” ê´€ë ¨ ---
        # ë³¼ ë¶€í’€ë¦¬ê¸° (ë³¼ì´ ë°”ê¹¥ìœ¼ë¡œ ë‚˜ê°)
        left_cheek_x = landmarks[self.LM["leftCheek"]].x
        right_cheek_x = landmarks[self.LM["rightCheek"]].x
        face_left_x = landmarks[self.LM["faceLeft"]].x
        face_right_x = landmarks[self.LM["faceRight"]].x
        
        cheek_puff = self._clamp(
            ((face_left_x - left_cheek_x) + (right_cheek_x - face_right_x)) 
            / face_width * 5.0
        )

        # ì½” ì°¡ê·¸ë¦¼
        nose_sneer_left = self._clamp(
            (landmarks[self.LM["noseLeft"]].y - landmarks[self.LM["noseTip"]].y)
            / face_height * 10.0
        )
        nose_sneer_right = self._clamp(
            (landmarks[self.LM["noseRight"]].y - landmarks[self.LM["noseTip"]].y)
            / face_height * 10.0
        )

        return Blendshapes(
            jawOpen=jaw_open,
            mouthSmileLeft=smile_left,
            mouthSmileRight=smile_right,
            mouthFrownLeft=frown_left,
            mouthFrownRight=frown_right,
            mouthPucker=mouth_pucker,
            mouthLeft=mouth_left,
            mouthRight=mouth_right,
            browInnerUp=brow_inner_up,
            browDownLeft=left_brow_down,
            browDownRight=right_brow_down,
            browOuterUpLeft=left_brow_outer_up,
            browOuterUpRight=right_brow_outer_up,
            eyeWideLeft=eye_wide_left,
            eyeWideRight=eye_wide_right,
            eyeSquintLeft=eye_squint_left,
            eyeSquintRight=eye_squint_right,
            eyeBlinkLeft=eye_blink_left,
            eyeBlinkRight=eye_blink_right,
            cheekPuff=cheek_puff,
            noseSneerLeft=nose_sneer_left,
            noseSneerRight=nose_sneer_right,
        )

    @staticmethod
    def _clamp(value: float, min_val: float = 0.0, max_val: float = 1.0) -> float:
        """ê°’ì„ min_valê³¼ max_val ì‚¬ì´ë¡œ ì œí•œ."""
        return max(min_val, min(max_val, value))

    # =========================================================================
    # ë°ì´í„° ë³‘í•©
    # =========================================================================

    def _merge_features(
        self,
        video_features: list[VideoFeatures],
        audio_features: list[AudioFeatures],
        fps: float,
    ) -> list[FrameData]:
        """
        ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ íŠ¹ì„±ì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë³‘í•©.
        """
        frame_duration_ms = 1000.0 / fps
        n_frames = min(len(video_features), len(audio_features))

        frames = []
        for i in range(n_frames):
            timestamp_ms = int(i * frame_duration_ms)
            
            frame = FrameData(
                timestamp_ms=timestamp_ms,
                video=video_features[i] if i < len(video_features) else None,
                audio=audio_features[i] if i < len(audio_features) else None,
            )
            frames.append(frame)

        return frames


# =============================================================================
# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (Lazy Loading)
# =============================================================================

_reference_analysis_service: Optional[ReferenceAnalysisService] = None


def get_reference_analysis_service() -> ReferenceAnalysisService:
    """ReferenceAnalysisService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
    global _reference_analysis_service
    if _reference_analysis_service is None:
        _reference_analysis_service = ReferenceAnalysisService()
    return _reference_analysis_service


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸ìš©
# =============================================================================

if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python reference_analysis_service.py <video_path> [actor_id]")
        sys.exit(1)

    video_path = sys.argv[1]
    actor_id = sys.argv[2] if len(sys.argv) > 2 else "test_actor"

    service = get_reference_analysis_service()
    
    # ë¶„ì„ ì‹¤í–‰
    result = service.analyze(
        video_path=video_path,
        actor_id=actor_id,
        output_path=f"{actor_id}_analysis.msgpack",
    )

    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)
    print(f"  Actor ID: {result.actor_id}")
    print(f"  Duration: {result.duration_sec:.2f}ì´ˆ")
    print(f"  FPS: {result.fps}")
    print(f"  Frames: {result.frame_count}")
    print(f"  Has Audio: {result.has_audio}")
    print(f"  Has Video: {result.has_video}")

    # ìƒ˜í”Œ í”„ë ˆì„ ì¶œë ¥
    if result.frames:
        sample = result.frames[len(result.frames) // 2]
        print(f"\n  ìƒ˜í”Œ í”„ë ˆì„ (ì¤‘ê°„):")
        print(f"    Timestamp: {sample.timestamp_ms}ms")
        if sample.audio:
            print(f"    Pitch: {sample.audio.pitch:.2f}Hz")
            print(f"    Energy: {sample.audio.energy:.6f}")
            print(f"    MFCC[0]: {sample.audio.mfcc[0]:.4f}")
        if sample.video and sample.video.blendshapes:
            print(f"    JawOpen: {sample.video.blendshapes.jawOpen:.4f}")
            print(f"    SmileL: {sample.video.blendshapes.mouthSmileLeft:.4f}")
