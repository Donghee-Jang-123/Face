import torch
import torchaudio
import torchaudio.transforms as T
import librosa
import numpy as np
import scipy.signal
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor

class AudioService:
    def __init__(self):
        print("ğŸ”Š AudioService: ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        
        # 1. GPU ê°€ì† í™•ì¸
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = "superb/wav2vec2-base-superb-er"
        self.target_sample_rate = 16000 
        
        # 2. ëª¨ë¸ ë¡œë“œ
        try:
            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(self.model_name)
            self.model = Wav2Vec2ForSequenceClassification.from_pretrained(self.model_name)
            self.model.to(self.device)
            print(f"ğŸ”Š AudioService: ë¡œë”© ì™„ë£Œ! (Device: {self.device})")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    def _load_and_resample(self, file_path: str):
        # A. ë¡œë“œ
        waveform, sample_rate = torchaudio.load(file_path)
        
        # B. ë¦¬ìƒ˜í”Œë§
        if sample_rate != self.target_sample_rate:
            resampler = T.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)
            waveform = resampler(waveform)
            
        # C. Stereo -> Mono
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
            
        return waveform.squeeze().numpy()

    def _trim_silence(self, audio_array):
        # ë¬´ìŒ ì œê±° (20dB ê¸°ì¤€)
        trimmed_audio, _ = librosa.effects.trim(audio_array, top_db=20)
        return trimmed_audio

    def _filter_noise(self, audio_array):
        # 100Hz ì´í•˜ ë…¸ì´ì¦ˆ ì œê±°
        sos = scipy.signal.butter(10, 100, 'hp', fs=self.target_sample_rate, output='sos')
        filtered_audio = scipy.signal.sosfilt(sos, audio_array)
        return filtered_audio

    def analyze_emotion(self, file_path: str):
        try:
            # 1. ì „ì²˜ë¦¬
            raw_audio = self._load_and_resample(file_path)
            trimmed_audio = self._trim_silence(raw_audio)
            clean_audio = self._filter_noise(trimmed_audio)

            # ë„ˆë¬´ ì§§ìœ¼ë©´(0.1ì´ˆ ë¯¸ë§Œ) ì›ë³¸ ì‚¬ìš©
            if len(clean_audio) < 1600:
                clean_audio = raw_audio

            # 2. ëª¨ë¸ ì…ë ¥ ë³€í™˜ (ê¸¸ì´ ì œí•œ ì œê±°ë¨)
            inputs = self.processor(
                clean_audio, 
                sampling_rate=self.target_sample_rate, 
                return_tensors="pt", 
                padding=True
            )
            
            inputs = {key: val.to(self.device) for key, val in inputs.items()}

            # 3. ì¶”ë¡ 
            with torch.no_grad():
                logits = self.model(**inputs).logits
            
            # 4. ê²°ê³¼ í•´ì„
            probs = torch.nn.functional.softmax(logits, dim=-1)
            predicted_id = torch.argmax(logits, dim=-1).item()
            emotion = self.model.config.id2label[predicted_id]
            confidence = probs[0][predicted_id].item()
            
            return {
                "emotion": emotion,
                "confidence": round(confidence * 100, 2)
            }
            
        except Exception as e:
            print(f"âŒ Audio Analysis Error: {e}")
            return {"emotion": "neutral", "confidence": 0.0}

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ëŠ” lazy loadingìœ¼ë¡œ ë³€ê²½ (ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œ)
audio_service = None

def get_audio_service():
    global audio_service
    if audio_service is None:
        audio_service = AudioService()
    return audio_service
