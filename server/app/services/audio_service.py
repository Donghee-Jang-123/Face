import torch
import torchaudio
import librosa
import numpy as np
import scipy.signal
import os
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor
# .editorë¥¼ ì§€ìš°ê³  ì•„ë˜ì²˜ëŸ¼ ê¸¸ê²Œ ì¨ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤!
from moviepy.video.io.VideoFileClip import VideoFileClip

class AudioService:
    def __init__(self):
        print("ğŸ”Š AudioService: ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = "superb/wav2vec2-base-superb-er"
        self.target_sample_rate = 16000 
        
        try:
            self.processor = Wav2Vec2FeatureExtractor.from_pretrained(self.model_name)
            self.model = Wav2Vec2ForSequenceClassification.from_pretrained(self.model_name)
            self.model.to(self.device)
            print(f"ğŸ”Š AudioService: ë¡œë”© ì™„ë£Œ! (Device: {self.device})")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    def _load_and_resample(self, file_path: str):
        # [í•µì‹¬ ìˆ˜ì •] MoviePyë¥¼ ì‚¬ìš©í•˜ì—¬ ê°•ì œë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
        temp_wav = file_path + ".temp.wav"
        
        try:
            # 1. íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

            # 2. ë™ì˜ìƒ íŒŒì¼(.mp4)ì¸ ê²½ìš° MoviePyë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            if file_path.lower().endswith('.mp4') or file_path.lower().endswith('.avi') or file_path.lower().endswith('.mov'):
                try:
                    # VideoFileClipì„ ì‚¬ìš©í•´ ì˜¤ë””ì˜¤ë§Œ ë”°ë¡œ ì €ì¥
                    video = VideoFileClip(file_path)
                    if video.audio is None:
                        raise ValueError("ì´ ë™ì˜ìƒì—ëŠ” ì†Œë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # 16000Hzë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œ wav íŒŒì¼ ì €ì¥ (logger=Noneìœ¼ë¡œ ë¡œê·¸ ë„ê¸°)
                    video.audio.write_audiofile(temp_wav, fps=self.target_sample_rate, logger=None)
                    video.close()
                    
                    # ì €ì¥ëœ wav íŒŒì¼ì„ Librosaë¡œ ë¡œë“œ
                    audio_array, _ = librosa.load(temp_wav, sr=self.target_sample_rate, mono=True)
                    return audio_array
                    
                except Exception as e:
                    print(f"âš ï¸ MoviePy ì¶”ì¶œ ì‹¤íŒ¨, Librosa ì§ì ‘ ì‹œë„: {e}")
                    # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‹œë„
                    pass
                finally:
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì²­ì†Œ)
                    if os.path.exists(temp_wav):
                        try:
                            os.remove(temp_wav)
                        except:
                            pass

            # 3. ì¼ë°˜ ì˜¤ë””ì˜¤ íŒŒì¼ì´ê±°ë‚˜ MoviePy ì‹¤íŒ¨ ì‹œ Librosa ì‚¬ìš©
            audio_array, _ = librosa.load(file_path, sr=self.target_sample_rate, mono=True)
            return audio_array

        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë¡œë”© ìµœì¢… ì‹¤íŒ¨ ({file_path}): {e}")
            raise e

    def _trim_silence(self, audio_array):
        try:
            trimmed_audio, _ = librosa.effects.trim(audio_array, top_db=20)
            return trimmed_audio
        except:
            return audio_array

    def _filter_noise(self, audio_array):
        try:
            sos = scipy.signal.butter(10, 100, 'hp', fs=self.target_sample_rate, output='sos')
            filtered_audio = scipy.signal.sosfilt(sos, audio_array)
            return filtered_audio
        except:
            return audio_array

    def analyze_emotion(self, file_path: str):
        try:
            raw_audio = self._load_and_resample(file_path)
            trimmed_audio = self._trim_silence(raw_audio)
            clean_audio = self._filter_noise(trimmed_audio)

            if len(clean_audio) < 1600:
                clean_audio = raw_audio

            inputs = self.processor(
                clean_audio, 
                sampling_rate=self.target_sample_rate, 
                return_tensors="pt", 
                padding=True
            )
            
            inputs = {key: val.to(self.device) for key, val in inputs.items()}

            with torch.no_grad():
                logits = self.model(**inputs).logits
            
            probs = torch.nn.functional.softmax(logits, dim=-1)
            predicted_id = torch.argmax(logits, dim=-1).item()
            emotion = self.model.config.id2label[predicted_id]
            confidence = probs[0][predicted_id].item()
            
            return {
                "emotion": emotion,
                "confidence": round(confidence * 100, 2)
            }
            
        except Exception as e:
            print(f"âŒ Audio Analysis Error: {e}")
            return {"emotion": "neutral", "confidence": 0.0}

audio_service = None

def get_audio_service():
    global audio_service
    if audio_service is None:
        audio_service = AudioService()
    return audio_service