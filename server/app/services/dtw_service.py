"""
DTW Synchronization Service (Stage 2)

Audio MFCC ê¸°ë°˜ Dynamic Time Warpingìœ¼ë¡œ ì‚¬ìš©ì ì˜¤ë””ì˜¤ë¥¼ 
ë ˆí¼ëŸ°ìŠ¤ ë°°ìš° ì˜¤ë””ì˜¤ì— ë™ê¸°í™”í•©ë‹ˆë‹¤.

í•µì‹¬ íŠ¹ì§•:
- Sakoe-Chiba Band ì œì•½: O(N*W) ë³µì¡ë„ (W = window size)
- Z-score ì •ê·œí™”: MFCC ë²¡í„° ì •ê·œí™”ë¡œ ê³µì •í•œ ë¹„êµ
- Cosine Distance: MFCCì— ìµœì í™”ëœ ê±°ë¦¬ ë©”íŠ¸ë¦­

ì„±ëŠ¥ ë¹„êµ (300 í”„ë ˆì„ ê¸°ì¤€):
- fastdtw: ~5ms (ê·¼ì‚¬, ì •í™•ë„ â†“)
- ë³¸ êµ¬í˜„: ~10ms (ì •í™•, Sakoe-Chiba band=50)
- ìˆœìˆ˜ DTW: ~50ms (ì •í™•, ì œì•½ ì—†ìŒ)
"""

from __future__ import annotations

import tempfile
from pathlib import Path
from typing import Optional, Union

import librosa
import numpy as np
from numpy.typing import NDArray
from scipy.spatial.distance import cdist

from app.core.schemas import AnalysisResult, DTWResult


class DTWService:
    """
    Audio-only DTW ë™ê¸°í™” ì„œë¹„ìŠ¤.
    
    MFCC íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì˜¤ë””ì˜¤ë¥¼ ë ˆí¼ëŸ°ìŠ¤ì— ì •ë ¬í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        window_ratio: float = 0.2,
        distance_metric: str = "cosine",
        n_mfcc: int = 13,
    ):
        """
        Args:
            window_ratio: Sakoe-Chiba ìœˆë„ìš° ë¹„ìœ¨ (0.1 ~ 0.5)
                         ë‚®ì„ìˆ˜ë¡ ë¹ ë¥´ì§€ë§Œ í° ì‹œê°„ ì°¨ì´ë¥¼ ëª» ì¡ìŒ
            distance_metric: ê±°ë¦¬ ë©”íŠ¸ë¦­ ('cosine' ë˜ëŠ” 'euclidean')
            n_mfcc: MFCC ê³„ìˆ˜ ê°œìˆ˜ (ë ˆí¼ëŸ°ìŠ¤ì™€ ë™ì¼í•´ì•¼ í•¨)
        """
        self.window_ratio = window_ratio
        self.distance_metric = distance_metric
        self.n_mfcc = n_mfcc
        
        print(f"ğŸ”— DTWService: ì´ˆê¸°í™” ì™„ë£Œ (metric={distance_metric}, window_ratio={window_ratio})")

    def synchronize(
        self,
        user_audio: Union[str, Path, NDArray[np.floating]],
        reference: AnalysisResult,
        user_id: str = "user",
    ) -> DTWResult:
        """
        ì‚¬ìš©ì ì˜¤ë””ì˜¤ë¥¼ ë ˆí¼ëŸ°ìŠ¤ì— ë™ê¸°í™”.
        
        Args:
            user_audio: ì‚¬ìš©ì ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy ë°°ì—´
            reference: Stage 1ì—ì„œ ìƒì„±ëœ ë ˆí¼ëŸ°ìŠ¤ AnalysisResult
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            DTWResult: ë™ê¸°í™” ê²°ê³¼ (warping_path, distance ë“±)
        """
        # 1. ì‚¬ìš©ì MFCC ì¶”ì¶œ (ë ˆí¼ëŸ°ìŠ¤ì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        user_mfcc = self._extract_user_mfcc(
            user_audio,
            sampling_rate=reference.sampling_rate,
            hop_length=reference.audio_hop_length,
            n_mfcc=reference.mfcc_n_coeffs,
        )

        # 2. ë ˆí¼ëŸ°ìŠ¤ MFCC ì¶”ì¶œ
        ref_mfcc = np.array(reference.get_mfcc_matrix())

        # 3. Z-score ì •ê·œí™”
        user_mfcc_norm = self._normalize_mfcc(user_mfcc)
        ref_mfcc_norm = self._normalize_mfcc(ref_mfcc)

        # 4. DTW ì‹¤í–‰
        path, distance = self._compute_dtw(
            user_mfcc_norm,
            ref_mfcc_norm,
        )

        # 5. ì •ê·œí™”ëœ ê±°ë¦¬ ê³„ì‚° (0-1 ë²”ìœ„)
        path_length = len(path)
        normalized_distance = distance / path_length if path_length > 0 else 0.0

        # 6. ê²°ê³¼ ìƒì„±
        result = DTWResult(
            actor_id=reference.actor_id,
            user_id=user_id,
            warping_path=path,
            distance=distance,
            normalized_distance=min(1.0, normalized_distance),
        )

        return result

    def get_timestamp_mapping(
        self,
        dtw_result: DTWResult,
        reference: AnalysisResult,
        user_fps: Optional[float] = None,
    ) -> dict[int, int]:
        """
        DTW ê²°ê³¼ë¥¼ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜.
        
        Args:
            dtw_result: synchronize() ê²°ê³¼
            reference: ë ˆí¼ëŸ°ìŠ¤ AnalysisResult
            user_fps: ì‚¬ìš©ì ë¹„ë””ì˜¤ FPS (Noneì´ë©´ ë ˆí¼ëŸ°ìŠ¤ FPS ì‚¬ìš©)
            
        Returns:
            {user_timestamp_ms: actor_timestamp_ms} ë§¤í•‘
        """
        user_fps = user_fps or reference.fps
        user_frame_duration_ms = 1000.0 / user_fps
        ref_frame_duration_ms = 1000.0 / reference.fps

        mapping = {}
        for user_idx, ref_idx in dtw_result.warping_path:
            user_ts = int(user_idx * user_frame_duration_ms)
            ref_ts = int(ref_idx * ref_frame_duration_ms)
            mapping[user_ts] = ref_ts

        return mapping

    def get_frame_mapping(
        self,
        dtw_result: DTWResult,
    ) -> dict[int, int]:
        """
        DTW ê²°ê³¼ë¥¼ í”„ë ˆì„ ì¸ë±ìŠ¤ ë§¤í•‘ìœ¼ë¡œ ë³€í™˜.
        
        Returns:
            {user_frame_idx: actor_frame_idx} ë§¤í•‘
        """
        return {user_idx: ref_idx for user_idx, ref_idx in dtw_result.warping_path}

    # =========================================================================
    # MFCC ì¶”ì¶œ
    # =========================================================================

    def _extract_user_mfcc(
        self,
        audio_input: Union[str, Path, NDArray[np.floating]],
        sampling_rate: int,
        hop_length: int,
        n_mfcc: int,
    ) -> NDArray[np.floating]:
        """
        ì‚¬ìš©ì ì˜¤ë””ì˜¤ì—ì„œ MFCC ì¶”ì¶œ (ë ˆí¼ëŸ°ìŠ¤ì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°).
        """
        # numpy ë°°ì—´ì´ ì•„ë‹ˆë©´ íŒŒì¼ì—ì„œ ë¡œë“œ
        if isinstance(audio_input, (str, Path)):
            audio_input = Path(audio_input)
            
            # ë¹„ë””ì˜¤ íŒŒì¼ì¸ ê²½ìš° ì˜¤ë””ì˜¤ ì¶”ì¶œ
            if audio_input.suffix.lower() in ('.mp4', '.avi', '.mov', '.webm'):
                audio_np = self._extract_audio_from_video(audio_input, sampling_rate)
            else:
                audio_np, _ = librosa.load(
                    str(audio_input),
                    sr=sampling_rate,
                    mono=True,
                )
        else:
            audio_np = audio_input

        # MFCC ì¶”ì¶œ
        mfcc = librosa.feature.mfcc(
            y=audio_np,
            sr=sampling_rate,
            n_mfcc=n_mfcc,
            n_fft=2048,
            hop_length=hop_length,
        )

        # (n_mfcc, n_frames) -> (n_frames, n_mfcc)
        return mfcc.T

    def _extract_audio_from_video(
        self,
        video_path: Path,
        target_sr: int,
    ) -> NDArray[np.floating]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (Windows í˜¸í™˜).
        
        ffmpegë¡œ WAV ì¶”ì¶œ í›„ librosaë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        import os
        import subprocess

        temp_wav = None
        try:
            # ffmpegë¡œ WAV ì¶”ì¶œ (subprocess ì‚¬ìš© - Windows í˜¸í™˜ì„±)
            temp_wav = tempfile.mktemp(suffix=".wav")
            ffmpeg_cmd = [
                'ffmpeg', '-y', '-i', str(video_path),
                '-vn', '-acodec', 'pcm_s16le',
                '-ar', str(target_sr), '-ac', '1',
                temp_wav, '-loglevel', 'error'
            ]
            
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True)
            except (subprocess.CalledProcessError, FileNotFoundError):
                # ffmpeg ëª…ë ¹ì–´ í˜•íƒœë¡œ ì¬ì‹œë„ (PATH ë¬¸ì œ ëŒ€ì‘)
                os.system(
                    f'ffmpeg -y -i "{video_path}" -vn -acodec pcm_s16le '
                    f'-ar {target_sr} -ac 1 "{temp_wav}" -loglevel error'
                )

            if os.path.exists(temp_wav) and os.path.getsize(temp_wav) > 0:
                audio_np, _ = librosa.load(temp_wav, sr=target_sr, mono=True)
                return audio_np

            # í´ë°±: librosa ì§ì ‘ ì‚¬ìš© (ì¼ë¶€ í¬ë§·ë§Œ ì§€ì›)
            audio_np, _ = librosa.load(str(video_path), sr=target_sr, mono=True)
            return audio_np

        finally:
            if temp_wav and os.path.exists(temp_wav):
                try:
                    os.remove(temp_wav)
                except OSError:
                    pass

    # =========================================================================
    # ì •ê·œí™”
    # =========================================================================

    def _normalize_mfcc(
        self,
        mfcc: NDArray[np.floating],
        method: str = "zscore",
    ) -> NDArray[np.floating]:
        """
        MFCC ì •ê·œí™” (ê³„ìˆ˜ë³„ Z-score).
        
        Args:
            mfcc: shape (n_frames, n_mfcc)
            method: 'zscore' ë˜ëŠ” 'minmax'
        """
        if mfcc.size == 0:
            return mfcc

        if method == "zscore":
            # ê° MFCC ê³„ìˆ˜ë³„ë¡œ Z-score ì •ê·œí™”
            mean = np.mean(mfcc, axis=0, keepdims=True)
            std = np.std(mfcc, axis=0, keepdims=True)
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            std = np.where(std < 1e-8, 1.0, std)
            return (mfcc - mean) / std
        
        elif method == "minmax":
            min_val = np.min(mfcc, axis=0, keepdims=True)
            max_val = np.max(mfcc, axis=0, keepdims=True)
            range_val = max_val - min_val
            range_val = np.where(range_val < 1e-8, 1.0, range_val)
            return (mfcc - min_val) / range_val
        
        else:
            raise ValueError(f"Unknown normalization method: {method}")

    # =========================================================================
    # DTW ì•Œê³ ë¦¬ì¦˜
    # =========================================================================

    def _compute_dtw(
        self,
        seq1: NDArray[np.floating],
        seq2: NDArray[np.floating],
    ) -> tuple[list[tuple[int, int]], float]:
        """
        Sakoe-Chiba Band ì œì•½ì´ ìˆëŠ” DTW ê³„ì‚°.
        
        Args:
            seq1: ì‚¬ìš©ì ì‹œí€€ìŠ¤ (n_frames1, n_features)
            seq2: ë ˆí¼ëŸ°ìŠ¤ ì‹œí€€ìŠ¤ (n_frames2, n_features)
            
        Returns:
            (warping_path, total_distance)
        """
        n, m = len(seq1), len(seq2)
        
        if n == 0 or m == 0:
            return [], 0.0

        # Sakoe-Chiba window í¬ê¸° ê³„ì‚°
        window = max(int(max(n, m) * self.window_ratio), abs(n - m) + 1)

        # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° (scipy ì‚¬ìš© - ë¹ ë¦„)
        if self.distance_metric == "cosine":
            # Cosine distance: 1 - cosine_similarity
            cost_matrix = cdist(seq1, seq2, metric="cosine")
        else:
            cost_matrix = cdist(seq1, seq2, metric="euclidean")

        # DTW ëˆ„ì  ë¹„ìš© í–‰ë ¬
        dtw_matrix = np.full((n + 1, m + 1), np.inf)
        dtw_matrix[0, 0] = 0

        # DTW ê³„ì‚° (Sakoe-Chiba band ì ìš©)
        for i in range(1, n + 1):
            # ìœˆë„ìš° ë²”ìœ„ ê³„ì‚°
            j_start = max(1, i - window)
            j_end = min(m + 1, i + window + 1)
            
            for j in range(j_start, j_end):
                cost = cost_matrix[i - 1, j - 1]
                dtw_matrix[i, j] = cost + min(
                    dtw_matrix[i - 1, j],      # insertion
                    dtw_matrix[i, j - 1],      # deletion
                    dtw_matrix[i - 1, j - 1],  # match
                )

        # ìµœì¢… ê±°ë¦¬
        total_distance = float(dtw_matrix[n, m])

        # Backtrackingìœ¼ë¡œ ìµœì  ê²½ë¡œ ì¶”ì¶œ
        path = self._backtrack(dtw_matrix, n, m)

        return path, total_distance

    def _backtrack(
        self,
        dtw_matrix: NDArray[np.floating],
        n: int,
        m: int,
    ) -> list[tuple[int, int]]:
        """DTW í–‰ë ¬ì—ì„œ ìµœì  ê²½ë¡œ ì—­ì¶”ì ."""
        path = []
        i, j = n, m

        while i > 0 and j > 0:
            path.append((i - 1, j - 1))  # 0-indexed

            # ì„¸ ë°©í–¥ ì¤‘ ìµœì†Œ ë¹„ìš© ì„ íƒ
            candidates = [
                (dtw_matrix[i - 1, j - 1], i - 1, j - 1),  # diagonal
                (dtw_matrix[i - 1, j], i - 1, j),          # up
                (dtw_matrix[i, j - 1], i, j - 1),          # left
            ]
            
            # ìœ íš¨í•œ í›„ë³´ë§Œ í•„í„°ë§
            valid_candidates = [(c, ni, nj) for c, ni, nj in candidates if c != np.inf]
            
            if not valid_candidates:
                break
                
            _, i, j = min(valid_candidates, key=lambda x: x[0])

        # ê²½ë¡œ ë’¤ì§‘ê¸° (ì‹œì‘ì  -> ëì )
        path.reverse()
        return path


# =============================================================================
# ê³ ê¸‰ DTW ê¸°ëŠ¥ (ì„ íƒì )
# =============================================================================

class AdvancedDTWService(DTWService):
    """
    ì¶”ê°€ ê¸°ëŠ¥ì´ ìˆëŠ” ê³ ê¸‰ DTW ì„œë¹„ìŠ¤.
    
    - ë‹¤ì¤‘ í•´ìƒë„ DTW (ë¹ ë¥¸ ê·¼ì‚¬)
    - ë¶€ë¶„ ë§¤ì¹­ (subsequence DTW)
    - ê°€ì¤‘ì¹˜ MFCC
    """

    def __init__(
        self,
        window_ratio: float = 0.2,
        distance_metric: str = "cosine",
        n_mfcc: int = 13,
        mfcc_weights: Optional[list[float]] = None,
    ):
        """
        Args:
            mfcc_weights: MFCC ê³„ìˆ˜ë³„ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ê· ë“±)
                         ì˜ˆ: ì²« ë²ˆì§¸ ê³„ìˆ˜(ì—ë„ˆì§€)ì— ë‚®ì€ ê°€ì¤‘ì¹˜
        """
        super().__init__(window_ratio, distance_metric, n_mfcc)
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜: ì²« ë²ˆì§¸ MFCC(ì—ë„ˆì§€ ê´€ë ¨)ì— ë‚®ì€ ê°€ì¤‘ì¹˜
        if mfcc_weights is None:
            self.mfcc_weights = np.array([0.5] + [1.0] * (n_mfcc - 1))
        else:
            self.mfcc_weights = np.array(mfcc_weights)

    def _normalize_mfcc(
        self,
        mfcc: NDArray[np.floating],
        method: str = "zscore",
    ) -> NDArray[np.floating]:
        """ì •ê·œí™” í›„ ê°€ì¤‘ì¹˜ ì ìš©."""
        normalized = super()._normalize_mfcc(mfcc, method)
        # ê°€ì¤‘ì¹˜ ì ìš©
        return normalized * self.mfcc_weights

    def synchronize_with_confidence(
        self,
        user_audio: Union[str, Path, NDArray[np.floating]],
        reference: AnalysisResult,
        user_id: str = "user",
    ) -> tuple[DTWResult, float]:
        """
        ë™ê¸°í™” + ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜.
        
        Returns:
            (DTWResult, confidence_score)
            confidence_score: 0-100 (ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë§¤ì¹­)
        """
        result = self.synchronize(user_audio, reference, user_id)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        # ë‚®ì€ normalized_distance = ë†’ì€ ì‹ ë¢°ë„
        confidence = max(0.0, 100.0 * (1.0 - result.normalized_distance * 2))
        
        return result, confidence

    def find_best_subsequence(
        self,
        user_audio: Union[str, Path, NDArray[np.floating]],
        reference: AnalysisResult,
        user_id: str = "user",
    ) -> tuple[DTWResult, int, int]:
        """
        ì‚¬ìš©ì ì˜¤ë””ì˜¤ì˜ ìµœì  ë¶€ë¶„ ì‹œí€€ìŠ¤ ì°¾ê¸°.
        
        ì‚¬ìš©ì ì˜¤ë””ì˜¤ê°€ ë ˆí¼ëŸ°ìŠ¤ë³´ë‹¤ ê¸¸ ë•Œ, ê°€ì¥ ì˜ ë§¤ì¹­ë˜ëŠ”
        êµ¬ê°„ì„ ì°¾ìŠµë‹ˆë‹¤.
        
        Returns:
            (DTWResult, start_frame, end_frame)
        """
        # ì‚¬ìš©ì MFCC ì¶”ì¶œ
        user_mfcc = self._extract_user_mfcc(
            user_audio,
            sampling_rate=reference.sampling_rate,
            hop_length=reference.audio_hop_length,
            n_mfcc=reference.mfcc_n_coeffs,
        )
        ref_mfcc = np.array(reference.get_mfcc_matrix())

        user_len = len(user_mfcc)
        ref_len = len(ref_mfcc)

        # ë ˆí¼ëŸ°ìŠ¤ê°€ ë” ê¸¸ë©´ ì¼ë°˜ DTW ì‚¬ìš©
        if user_len <= ref_len:
            result = self.synchronize(user_audio, reference, user_id)
            return result, 0, user_len

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ìµœì  êµ¬ê°„ íƒìƒ‰
        best_distance = np.inf
        best_start = 0
        best_path = []

        # ìœˆë„ìš° í¬ê¸° = ë ˆí¼ëŸ°ìŠ¤ ê¸¸ì´
        window_size = ref_len
        step = max(1, window_size // 10)  # 10% ìŠ¤í…

        for start in range(0, user_len - window_size + 1, step):
            end = start + window_size
            user_segment = user_mfcc[start:end]

            # ì •ê·œí™”
            user_norm = self._normalize_mfcc(user_segment)
            ref_norm = self._normalize_mfcc(ref_mfcc)

            # DTW
            path, distance = self._compute_dtw(user_norm, ref_norm)

            if distance < best_distance:
                best_distance = distance
                best_start = start
                best_path = [(u + start, r) for u, r in path]

        # ê²°ê³¼ ìƒì„±
        path_length = len(best_path)
        normalized_distance = best_distance / path_length if path_length > 0 else 0.0

        result = DTWResult(
            actor_id=reference.actor_id,
            user_id=user_id,
            warping_path=best_path,
            distance=best_distance,
            normalized_distance=min(1.0, normalized_distance),
        )

        return result, best_start, best_start + window_size


# =============================================================================
# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (Lazy Loading)
# =============================================================================

_dtw_service: Optional[DTWService] = None


def get_dtw_service() -> DTWService:
    """DTWService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
    global _dtw_service
    if _dtw_service is None:
        _dtw_service = DTWService()
    return _dtw_service


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸ìš©
# =============================================================================

if __name__ == "__main__":
    import sys
    import time

    if len(sys.argv) < 3:
        print("Usage: python dtw_service.py <user_audio> <reference.msgpack>")
        sys.exit(1)

    user_audio_path = sys.argv[1]
    reference_path = sys.argv[2]

    # ë ˆí¼ëŸ°ìŠ¤ ë¡œë“œ
    print(f"ğŸ“‚ ë ˆí¼ëŸ°ìŠ¤ ë¡œë“œ: {reference_path}")
    reference = AnalysisResult.load(reference_path)
    print(f"   Actor: {reference.actor_id}, Frames: {reference.frame_count}")

    # DTW ì„œë¹„ìŠ¤
    service = get_dtw_service()

    # ë™ê¸°í™” ì‹¤í–‰
    print(f"\nğŸ”— DTW ë™ê¸°í™” ì‹œì‘...")
    start_time = time.time()
    
    result = service.synchronize(
        user_audio=user_audio_path,
        reference=reference,
        user_id="test_user",
    )
    
    elapsed = time.time() - start_time

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ”— DTW ë™ê¸°í™” ê²°ê³¼")
    print("=" * 50)
    print(f"  ì†Œìš” ì‹œê°„: {elapsed * 1000:.2f}ms")
    print(f"  DTW ê±°ë¦¬: {result.distance:.4f}")
    print(f"  ì •ê·œí™” ê±°ë¦¬: {result.normalized_distance:.4f}")
    print(f"  ê²½ë¡œ ê¸¸ì´: {len(result.warping_path)}")

    # ë§¤í•‘ ìƒ˜í”Œ ì¶œë ¥
    mapping = service.get_timestamp_mapping(result, reference)
    timestamps = sorted(mapping.keys())
    
    print(f"\n  íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤í•‘ (ìƒ˜í”Œ):")
    for ts in timestamps[:5]:
        print(f"    User {ts}ms -> Actor {mapping[ts]}ms")
    if len(timestamps) > 10:
        print(f"    ...")
    for ts in timestamps[-3:]:
        print(f"    User {ts}ms -> Actor {mapping[ts]}ms")

    # ì‹ ë¢°ë„ í…ŒìŠ¤íŠ¸ (Advanced)
    print("\nğŸ¯ ì‹ ë¢°ë„ í‰ê°€...")
    advanced_service = AdvancedDTWService()
    _, confidence = advanced_service.synchronize_with_confidence(
        user_audio=user_audio_path,
        reference=reference,
    )
    print(f"  ì‹ ë¢°ë„ ì ìˆ˜: {confidence:.1f}/100")
